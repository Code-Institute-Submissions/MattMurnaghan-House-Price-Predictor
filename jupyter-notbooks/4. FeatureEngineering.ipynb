{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform feature engineering for regression features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data from the cleaned data outputs\n",
    "* Analyse "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set the working directory for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in from the outputs section of the last notebook. We can drop the SalePrice column as it is the target varialbe in this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"outputs/datasets/cleaned/clean_house_price_records.csv\")\n",
    "train_set = pd.read_csv(\"outputs/datasets/cleaned/train_set.csv\")\n",
    "test_set = pd.read_csv(\"outputs/datasets/cleaned/test_set.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function below allows us to apply a set of defined transformers to the data.\n",
    "* Requires that only either numerical or categorical data is passed, not a mixture of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import transformation as vt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def FeatureEngineeringAnalysis(df,analysis_type=None):\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  - used for quick feature engineering on numerical and categorical variables\n",
    "  to decide which transformation can better transform the distribution shape \n",
    "  - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
    "\n",
    "  \"\"\"\n",
    "  check_missing_values(df)\n",
    "  allowed_types= ['numerical', 'ordinal_encoder',  'outlier_winsorizer']\n",
    "  check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
    "  list_column_transformers = define_list_column_transformers(analysis_type)\n",
    "  \n",
    "  \n",
    "  # Loop over each variable and engineer the data according to the analysis type\n",
    "  df_feat_eng = pd.DataFrame([])\n",
    "  for column in df.columns:\n",
    "    # create additional columns (column_method) to apply the methods\n",
    "    df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
    "    for method in list_column_transformers:\n",
    "      df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
    "      \n",
    "    # Apply transformers in respectives column_transformers\n",
    "    df_feat_eng, list_applied_transformers = apply_transformers(analysis_type, df_feat_eng, column)\n",
    "\n",
    "    # For each variable, assess how the transformations perform\n",
    "    transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng)\n",
    "\n",
    "  return df_feat_eng\n",
    "\n",
    "\n",
    "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
    "  ### Check analyis type\n",
    "  if analysis_type == None:\n",
    "    raise SystemExit(f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
    "  if analysis_type not in allowed_types:\n",
    "      raise SystemExit(f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
    "\n",
    "def check_missing_values(df):\n",
    "  if df.isna().sum().sum() != 0:\n",
    "    raise SystemExit(\n",
    "        f\"There is missing values in your dataset. Please handle that before getting into feature engineering.\")\n",
    "\n",
    "\n",
    "\n",
    "def define_list_column_transformers(analysis_type):\n",
    "  ### Set suffix colummns acording to analysis_type\n",
    "  if analysis_type=='numerical':\n",
    "    list_column_transformers = [\"log_e\",\"log_10\",\"reciprocal\", \"power\",\"box_cox\",\"yeo_johnson\"]\n",
    "  \n",
    "  elif analysis_type=='ordinal_encoder':\n",
    "    list_column_transformers = [\"ordinal_encoder\"]\n",
    "\n",
    "  elif analysis_type=='outlier_winsorizer':\n",
    "    list_column_transformers = ['iqr']\n",
    "\n",
    "  return list_column_transformers\n",
    "\n",
    "\n",
    "\n",
    "def apply_transformers(analysis_type, df_feat_eng, column):\n",
    "\n",
    "\n",
    "  for col in df_feat_eng.select_dtypes(include='category').columns:\n",
    "    df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
    "\n",
    "\n",
    "  if analysis_type=='numerical':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,column)\n",
    "  \n",
    "  elif analysis_type=='outlier_winsorizer':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_OutlierWinsorizer(df_feat_eng,column)\n",
    "\n",
    "  elif analysis_type=='ordinal_encoder':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_CategoricalEncoder(df_feat_eng,column)\n",
    "\n",
    "  return df_feat_eng,list_applied_transformers\n",
    "\n",
    "\n",
    "\n",
    "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
    "  # For each variable, assess how the transformations perform\n",
    "  print(f\"* Variable Analyzed: {column}\")\n",
    "  print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
    "  for col in [column] + list_applied_transformers:\n",
    "    \n",
    "    if analysis_type!='ordinal_encoder':\n",
    "      DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "    \n",
    "    else:\n",
    "      if col == column: \n",
    "        DiagnosticPlots_Categories(df_feat_eng, col)\n",
    "      else:\n",
    "        DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
    "  plt.figure(figsize=(20, 5))\n",
    "  sns.countplot(data=df_feat_eng, x=col,palette=['#432371'],order = df_feat_eng[col].value_counts().index)\n",
    "  plt.xticks(rotation=90) \n",
    "  plt.suptitle(f\"{col}\", fontsize=30,y=1.05)        \n",
    "  plt.show();\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Numerical(df, variable):\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "  sns.histplot(data=df, x=variable, kde=True,element=\"step\",ax=axes[0]) \n",
    "  stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
    "  sns.boxplot(x=df[variable],ax=axes[2])\n",
    "  \n",
    "  axes[0].set_title('Histogram')\n",
    "  axes[1].set_title('QQ Plot')\n",
    "  axes[2].set_title('Boxplot')\n",
    "  fig.suptitle(f\"{variable}\", fontsize=30,y=1.05)\n",
    "  plt.show();\n",
    "\n",
    "\n",
    "def FeatEngineering_CategoricalEncoder(df_feat_eng,column):\n",
    "  list_methods_worked = []\n",
    "  try:  \n",
    "    encoder= OrdinalEncoder(encoding_method='arbitrary', variables = [f\"{column}_ordinal_encoder\"])\n",
    "    df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
    "  \n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_ordinal_encoder\"],axis=1,inplace=True)\n",
    "    \n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_OutlierWinsorizer(df_feat_eng,column):\n",
    "  list_methods_worked = []\n",
    "\n",
    "  ### Winsorizer iqr\n",
    "  try: \n",
    "    disc=Winsorizer(\n",
    "        capping_method='iqr', tail='both', fold=1.5, variables = [f\"{column}_iqr\"])\n",
    "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_iqr\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FeatEngineering_Numerical(df_feat_eng,column):\n",
    "\n",
    "  list_methods_worked = []\n",
    "\n",
    "  ### LogTransformer base e\n",
    "  try: \n",
    "    lt = vt.LogTransformer(variables = [f\"{column}_log_e\"])\n",
    "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_log_e\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_log_e\"],axis=1,inplace=True)\n",
    "\n",
    "    ### LogTransformer base 10\n",
    "  try: \n",
    "    lt = vt.LogTransformer(variables = [f\"{column}_log_10\"],base='10')\n",
    "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_log_10\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_log_10\"],axis=1,inplace=True)\n",
    "\n",
    "  ### ReciprocalTransformer\n",
    "  try:\n",
    "    rt = vt.ReciprocalTransformer(variables = [f\"{column}_reciprocal\"])\n",
    "    df_feat_eng =  rt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_reciprocal\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_reciprocal\"],axis=1,inplace=True)\n",
    "\n",
    "  ### PowerTransformer\n",
    "  try:\n",
    "    pt = vt.PowerTransformer(variables = [f\"{column}_power\"])\n",
    "    df_feat_eng = pt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_power\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_power\"],axis=1,inplace=True)\n",
    "\n",
    "  ### BoxCoxTransformer\n",
    "  try:\n",
    "    bct = vt.BoxCoxTransformer(variables = [f\"{column}_box_cox\"])\n",
    "    df_feat_eng = bct.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_box_cox\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_box_cox\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  ### YeoJohnsonTransformer\n",
    "  try:\n",
    "    yjt = vt.YeoJohnsonTransformer(variables = [f\"{column}_yeo_johnson\"])\n",
    "    df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
    "  except:\n",
    "        df_feat_eng.drop([f\"{column}_yeo_johnson\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  return df_feat_eng,list_methods_worked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is taken from the codeinstitute modules for feature engineering. It creates a heatmap for correlation scores and PPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                         linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\")\n",
    "    df_corr_pearson = df.corr(method=\"pearson\")\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some of the data is type int, but is actually categorical.\n",
    "* We can change it to type object using the astype() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['GarageFinish', 'BsmtFinType1', 'GarageYrBlt', 'BsmtExposure', 'OverallCond', 'OverallQual', 'KitchenQual']\n",
    "df[categorical_variables]=df[categorical_variables].astype('object')\n",
    "df_feat_eng = train_set.copy()\n",
    "df_feat_eng[categorical_variables]=df_feat_eng[categorical_variables].astype('object')\n",
    "df_numerical = df_feat_eng.select_dtypes(include=['int64','float64'])\n",
    "df_categorical = df_feat_eng.select_dtypes(include=['object'])\n",
    "# train_set[categorical_variables]=train_set[categorical_variables].astype('object')\n",
    "# test_set[categorical_variables]=test_set[categorical_variables].astype('object')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the new data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Analysis\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a numerical analysis on the varialbes in the dataset to see if we can normalise their distribution by using a series of numerical transformers and analysing the resulting data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df_feat_eng_numerical = FeatureEngineeringAnalysis(df=df_numerical,analysis_type='numerical')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Numerical Analysis\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the variables analysed, two distinct groups of transformations were created by the function, groups where:\n",
    "* Log_e, Log_10, Reciprocal, Power, Box_Cox, Yeo_Johnson\n",
    "* Power and Yeo_Johnson"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Group 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    "* 1stFlrSF\n",
    "* GrLivArea\n",
    "* LotArea\n",
    "* LotFrontage\n",
    "* YearBuilt\n",
    "* YearRemodAdd\n",
    "\n",
    "This group of numerical transformations was suggested by the CodeInstitue customer numerical analysis function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Breakdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1stFlrSF, GrLivArea, LotArea, LotFrontage**\n",
    "* These variables showed improvements under all transformations except reciprocal.\n",
    "* From the various plots, it can be seen that the histograms move closer to a normal distribution.\n",
    "* There is improvment in the QQ plots, with most points moving closer to the line of best fit.\n",
    "* The majority of outliers are eliminated, or their effect is reduced.\n",
    "\n",
    "We can conclude that the Yeo Johnson, Box Cox, Log_10 and Log_e numerical transformations can be considered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YearBuilt, YearRemodAdd**\n",
    "* These variables showed small improvements under the Yeo Johnson and Box Cox transformations.\n",
    "* From the various plots, it can be seen that the histograms move closer to a normal distribution.\n",
    "* There is marginal improvment in the QQ plots, with some points moving closer to the line of best fit.\n",
    "* The majority of outliers are eliminated, or their effect is reduced.\n",
    "\n",
    "We can conclude that the Yeo Johnson and Box Cox numerical transformations can be considered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Group 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    "* 2ndFlrSF\n",
    "* BsmtFinSF1\n",
    "* BsmtUnfSF\n",
    "* GarageArea\n",
    "* MasVnrArea\n",
    "* OpenPorchSF\n",
    "* TotalBsmtSF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Breakdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2ndFlrSf, BsmtFinSF1**\n",
    "* These variables had no noticeable improvement after transformation.\n",
    "* Some outliers were removed.\n",
    "\n",
    "We can conclude that these are good candidates for removal with smart feature selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BsmtUnfSF, GarageArea, MasVnrArea, OpenPorchSF, TotalBsmtSF**\n",
    "* These variables had small improvements after transformation, most noticeably with the power trasnformation.\n",
    "* Some histograms had a wider distribution.\n",
    "* QQ plots were slightly closer to the line of best fit.\n",
    "* Some outliers were removed.\n",
    "\n",
    "We can conclude that the Power numerical transformation can be considered."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a pipeline to handle the outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmartCorrelatedSelection Variables\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this analysis on the entire dataset to select only the variables with the highest correlation to the target variable: SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smart_corr = train_set.drop(['SalePrice'],axis=1)\n",
    "df_smart_corr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "selection_method = 'cardinality'\n",
    "corr_method = 'spearman'\n",
    "smart_corr_spearman = SmartCorrelatedSelection(variables=None, method=corr_method, threshold=0.60, selection_method=selection_method)\n",
    "\n",
    "smart_corr_spearman.fit_transform(df_smart_corr)\n",
    "smart_corr_spearman.correlated_feature_sets_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for pearson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_method = 'cardinality'\n",
    "corr_method = 'pearson'\n",
    "smart_corr_pearson = SmartCorrelatedSelection(variables=None, method=corr_method, threshold=0.60, selection_method=selection_method)\n",
    "\n",
    "smart_corr_pearson.fit_transform(df_smart_corr)\n",
    "smart_corr_pearson.correlated_feature_sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(train_set)\n",
    "%matplotlib inline\n",
    "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
    "                  df_corr_spearman = df_corr_spearman, \n",
    "                  pps_matrix = pps_matrix,\n",
    "                  CorrThreshold = 0.6, PPS_Threshold =0.5,\n",
    "                  figsize=(12*1,10*1), font_annot=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Transformation**\n",
    "* The ordinally encoded variables are BsmtExposure, BsmtFinType1, GarageFinish and KitchenQual.\n",
    "* The variables for numerical transformation have been split into 3 groups:\n",
    "    * 1stFlrSF, GrLivArea, LotArea, LotFrontage:\n",
    "        * Log e, Log 10, Yeo Johnson and Box Cox are considered.\n",
    "    * YearBuilt, YearRemodAdd:\n",
    "        * Yeo Johnson and Box Cox are considered.\n",
    "    * BsmtUnfSF, GarageArea, MasVnrArea, OpenPorchSF, TotalBsmtSF:\n",
    "        * Power is considered.\n",
    "    \n",
    "**Chosen Transformers**\n",
    "The following transformers will be employed:\n",
    "* ('LogTransform', vt.LogTransformer(variables=['1stFlrSF', 'GrLivArea', 'LotArea', 'LotFrontage',]))\n",
    "* ('YeoJohnsonTransform', vt.YeoJohnsonTransformer(variables=['YearBuilt', 'YearRemodAdd',]))\n",
    "* ('PowerTransform', vt.LogTransformer(variables=['BsmtUnfSF', 'GarageArea', 'MasVnrArea', 'OpenPorchSF', 'TotalBsmtSF',]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smart Correlation**\n",
    "* From the correlation study in the VariableStudy notebook, we discovered that the following variables are the most correlated variables with SalePrice.:\n",
    "    * 1stFlrSF\n",
    "    * GarageArea\n",
    "    * GarageFinish\n",
    "    * GrLivArea\n",
    "    * KitchenQual\n",
    "    * OverallQual\n",
    "    * TotalBsmtSF\n",
    "    * YearBuilt\n",
    "* From the SmartCorrelatedSelection, we can single out the overcorrelated variables in the dataset.\n",
    "* Variables we can drop:\n",
    "    * OverallCond\n",
    "    * 2ndFlrSF\n",
    "    * GarageYrBlt\n",
    "    * TotalBsmtSF\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the pipeline here are read it into the next jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine.transformation import PowerTransformer\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from feature_engine.transformation import LogTransformer\n",
    "\n",
    "selection_method = \"cardinality\"\n",
    "corr_method = \"spearman\"\n",
    "\n",
    "\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "      ('OrdinalEncoder',OrdinalEncoder(variables=['GarageFinish', 'BsmtFinType1', 'BsmtExposure', 'KitchenQual'])),\n",
    "      ('LogTransform', LogTransformer(variables=['1stFlrSF', 'GrLivArea', 'LotArea', 'LotFrontage',])),\n",
    "      ('YeoJohnsonTransform', YeoJohnsonTransformer(variables=['YearBuilt', 'YearRemodAdd',])),\n",
    "      ('PowerTransform', PowerTransformer(variables=['BsmtUnfSF', 'GarageArea', 'MasVnrArea', 'OpenPorchSF', 'TotalBsmtSF',])),\n",
    "      ('SmartFeatureSelection', SmartCorrelatedSelection(variables=None,\n",
    "                                                            method=corr_method,\n",
    "                                                            threshold=0.60,\n",
    "                                                            selection_method=selection_method)),\n",
    "                                                            ])\n",
    "\n",
    "feature_engineering_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = train_set.select_dtypes(include=['object']).columns\n",
    "train_set[categorical_variables]=train_set[categorical_variables].astype('category')\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fe = feature_engineering_pipeline.fit_transform(train_set.drop(['SalePrice'],axis=1),train_set['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_fe.shape)\n",
    "train_fe.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to create the directory for models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.makedirs(name='outputs/pipelines') # create outputs/datasets/collection folder\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(value=feature_engineering_pipeline ,filename=f\"outputs/pipelines/feature_engineering_pipeline.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
